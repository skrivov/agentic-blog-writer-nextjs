
========== /home/serg/node/agentic-blog-writer-nextjs/lib/types.ts ==========

// File: /lib/types.ts

/**
 * Defines the shape of a workflow object.
 */
export type WorkflowDefinition = {
  key: string
  handler: (params: HandlerParams) => Promise<string>
}

/**
 * Parameters passed to every workflow handler.
 */
export type HandlerParams = {
  prompt: string // The user’s text (either for creation or modification)
  rawMdx?: string // Optional: if provided, indicates modification mode
}


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflowsConfig.ts ==========

// File: /lib/workflowsConfig.ts

import { WorkflowDefinition } from './types'
import { runBasic } from './workflows/basic'
import { runChainOfThoughts } from './workflows/chainOfThoughts'

import { runWriterCritic } from './workflows/writerCritic'
import { runDeepResearch } from './workflows/deepResearch'

//The handlers can be used only on server side

export const workflows: WorkflowDefinition[] = [
  {
    key: 'basic',
    handler: runBasic,
  },
  {
    key: 'chainOfThoughts',
    handler: runChainOfThoughts,
  },
  {
    key: 'writerCritic',
    handler: runWriterCritic,
  },
  {
    key: 'deepResearch',
    handler: runDeepResearch,
  },
]


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflowsMetadata.ts ==========

// File: /lib/workflowsMetadata.ts

export type WorkflowMetadata = {
  key: string
  title: string
  promptInstruction: string
  applicability: 'new' | 'modification' | 'both'
}

//As the handlers can be used only on server side, we need to put metadata for the client
//in a special config file.

export const workflowsMetadata: WorkflowMetadata[] = [
  {
    key: 'basic',
    title: 'Basic',
    promptInstruction:
      'Enter your prompt for the LLM to generate a complete blog post in MDX format.',
    applicability: 'both',
  },
  {
    key: 'chainOfThoughts',
    title: 'Chain of Thoughts',
    promptInstruction:
      'Provide a topic to generate a detailed outline and a complete MDX blog post.',
    applicability: 'both',
  },
  {
    key: 'writerCritic',
    title: 'Writer-Critic',
    promptInstruction:
      'Enter a topic to generate a new blog post using an iterative writer and critic approach.',
    applicability: 'new',
  },
  {
    key: 'deepResearch',
    title: 'Deep Research',
    promptInstruction:
      'Enter a topic to generate a new, comprehensive blog post with deep research.',
    applicability: 'new',
  },
]


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/writerCritic/index.ts ==========

// File: /lib/workflows/writerCritic/index.ts

import { ChatOpenAI } from '@langchain/openai'
import { HumanMessage } from '@langchain/core/messages'
import { StateGraph, Annotation } from '@langchain/langgraph'
import type { HandlerParams } from '../../types'
import { writerInitialPromptV1, writerRevisionPromptV1, criticPromptV1 } from './prompts'
import { mdxFormattingInstructionsV1 } from '../../prompts/mdxFormatting'

// Constant to control how many writer/critic cycles to run (default is 1)
const MAX_ITERATIONS = 1

// Initialize the LLM (using GPT‑4 as in your other workflows)
const llm = new ChatOpenAI({
  model: 'gpt-4o-mini',
  temperature: 0.7,
  apiKey: process.env.OPENAI_API_KEY,
})

// Define the state for the workflow using LangGraph’s Annotation system
const StateAnnotation = Annotation.Root({
  topic: Annotation<string>(),
  blogDraft: Annotation<string>(),
  suggestions: Annotation<string>(),
  iteration: Annotation<number>({ value: (_, next) => next, default: () => 0 }),
  finalBlog: Annotation<string>(),
})

/**
 * Node: writerInitial
 * Creates an initial blog post based solely on the topic.
 */
async function writerInitial(state: typeof StateAnnotation.State) {
  const prompt = writerInitialPromptV1(state.topic)
  const response = await llm.invoke([new HumanMessage(prompt)])
  console.log('First Draft:', response.content)
  return { blogDraft: response.content }
}

/**
 * Node: critic
 * Reviews the current blog post and returns suggestions for improvement.
 */
async function critic(state: typeof StateAnnotation.State) {
  const prompt = criticPromptV1(state.blogDraft)
  const response = await llm.invoke([new HumanMessage(prompt)])
  console.log("Critic's response:", response.content)
  return { suggestions: response.content }
}

/**
 * Node: writerRevision
 * Revises the blog post by incorporating the critic’s suggestions and increments the revision count.
 */
async function writerRevision(state: typeof StateAnnotation.State) {
  const prompt = writerRevisionPromptV1(state.blogDraft, state.suggestions)
  const response = await llm.invoke([new HumanMessage(prompt)])
  console.log('Writer revision:', response.content)
  return { blogDraft: response.content, iteration: state.iteration + 1 }
}

/**
 * Node: formatting
 * Formats the final blog post into a perfectly formatted MDX file with SEO‑friendly frontmatter.
 * Uses the mdxFormattingInstructionsV1 from /lib/prompts/mdxFormatting.ts.
 */
async function formatting(state: typeof StateAnnotation.State) {
  const instructions = mdxFormattingInstructionsV1()
  const prompt = `${instructions}\n\nBlog Post:\n${state.blogDraft}`
  const response = await llm.invoke([new HumanMessage(prompt)])
  return { finalBlog: response.content }
}

/**
 * Conditional edge function after writerInitial.
 * If MAX_ITERATIONS > 0, proceed to the critic; otherwise, skip directly to formatting.
 */
function afterWriterInitial(state: typeof StateAnnotation.State): string {
  return MAX_ITERATIONS > 0 ? 'critic' : 'formatting'
}

/**
 * Conditional edge function after writerRevision.
 * If the current iteration count is less than MAX_ITERATIONS, loop back to the critic node;
 * otherwise, proceed to formatting.
 */
function afterWriterRevision(state: typeof StateAnnotation.State): string {
  return state.iteration < MAX_ITERATIONS ? 'critic' : 'formatting'
}

// Build the LangGraph state graph for the writer‑critic workflow
const writerCriticGraph = new StateGraph(StateAnnotation)
  .addNode('writerInitial', writerInitial)
  .addNode('critic', critic)
  .addNode('writerRevision', writerRevision)
  .addNode('formatting', formatting)
  .addEdge('__start__', 'writerInitial')
  .addConditionalEdges('writerInitial', afterWriterInitial, {
    critic: 'critic',
    formatting: 'formatting',
  })
  .addEdge('critic', 'writerRevision')
  .addConditionalEdges('writerRevision', afterWriterRevision, {
    critic: 'critic',
    formatting: 'formatting',
  })
  .addEdge('formatting', '__end__')
  .compile()

/**
 * runWriterCritic:
 * Entry point for the workflow.
 * Receives a topic from the user (in the `prompt` property of HandlerParams)
 * and returns a fully formatted MDX blog post.
 */
export async function runWriterCritic({ prompt }: HandlerParams): Promise<string> {
  const state = await writerCriticGraph.invoke({ topic: prompt })
  return state.finalBlog
}


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/writerCritic/prompts.ts ==========

// File: /lib/workflows/writerCritic/prompts.ts

import { mdxFormattingInstructionsV1 } from '../../prompts/mdxFormatting'

/**
 * writerInitialPrompt:
 * Given a topic, returns a prompt to write an engaging blog post.
 */
export const writerInitialPromptV1 = (topic: string): string =>
  `Write an engaging, informative, and creative blog post on the topic "${topic}".
Ensure the post is well-structured with clear section headings and engaging content.
Output only the blog post content without any additional commentary.`

/**
 * writerRevisionPrompt:
 * Given the current blog post and the critic's suggestions, returns a prompt to revise the blog post.
 */
export const writerRevisionPromptV1 = (blog: string, suggestions: string): string =>
  `Revise the following blog post by incorporating the improvement suggestions provided.
  
Blog Post:
${blog}

Suggestions for Improvement:
${suggestions}

Output the revised blog post ensuring enhanced clarity, improved structure, and greater engagement.`

/**
 * criticPrompt:
 * Given a blog post, returns a prompt for a critic to review it and provide constructive feedback.
 */
export const criticPromptV1 = (blog: string): string =>
  `You are a seasoned blog critic. Review the following blog post and provide detailed, constructive feedback focused on clarity, structure, engagement, and depth.
  
Blog Post:
${blog}

Output only the constructive feedback with specific suggestions for improvement.`


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/basic/index.ts ==========

// File: /lib/workflows/basic/index.ts

import { ChatOpenAI } from '@langchain/openai'
import { HumanMessage } from '@langchain/core/messages'
import { RunnableSequence } from '@langchain/core/runnables'
import { StringOutputParser } from '@langchain/core/output_parsers'
import type { HandlerParams } from '../../types'
import { basicCreationPromptV1 } from './prompts'
import { modificationPromptV1 } from '../../prompts/changes'

/**
 * runBasic: Uses the provided prompt to generate a complete blog post in MDX format.
 * If rawMdx is provided, the common modification prompt is used.
 */
export async function runBasic({ prompt, rawMdx }: HandlerParams): Promise<string> {
  const model = new ChatOpenAI({
    model: 'gpt-4o-mini',
    temperature: 0.7,
    apiKey: process.env.OPENAI_API_KEY,
  })

  const chain = RunnableSequence.from([
    async () => {
      const compositePrompt = rawMdx
        ? modificationPromptV1(rawMdx, prompt)
        : basicCreationPromptV1(prompt)
      const response = await model.invoke([new HumanMessage(compositePrompt)])
      return { mdx: response.content }
    },
    (output: { mdx: string }) => output.mdx,
    new StringOutputParser(),
  ])

  return chain.invoke({})
}


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/basic/prompts.ts ==========

// File: /lib/workflows/basic/prompts.ts

import { mdxFormattingInstructionsV1 } from '../../prompts/mdxFormatting'

/**
 * basicCreationPromptV1:
 * Given the user input, returns a prompt to generate a new blog post in MDX format.
 */
export const basicCreationPromptV1 = (input: string): string => {
  return `Using the input below, generate a complete blog post in MDX format.
${mdxFormattingInstructionsV1()}

Input: ${input}`
}


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/deepResearch/index.ts ==========

// File: /lib/workflows/deepResearch/index.ts

import { ChatOpenAI } from '@langchain/openai'
import { HumanMessage } from '@langchain/core/messages'
import { StateGraph, Annotation } from '@langchain/langgraph'
import { StringOutputParser } from '@langchain/core/output_parsers'
import type { HandlerParams } from '../../types'
import {
  researchSummaryPromptV1,
  draftBlogPromptV1,
  refineBlogPromptV1,
  seoPromptV1,
  formatMdxPromptV1,
} from './prompts'

// Import the latest Tavily retriever
import { TavilySearchAPIRetriever } from '@langchain/community/retrievers/tavily_search_api'

// Instantiate your LLM (using GPT‑4)
const llm = new ChatOpenAI({
  model: 'gpt-4o-mini',
  temperature: 0.7,
  apiKey: process.env.OPENAI_API_KEY,
})

// Instantiate the TavilySearchAPIRetriever (set k to 3 or adjust as needed)
const retriever = new TavilySearchAPIRetriever({
  k: 3,
  // Optionally pass your API key directly:
  // apiKey: process.env.TAVILY_API_KEY,
})

// Set the number of iterative refinement cycles (default is 1)
const MAX_ITERATIONS = 1

// Define the state using LangGraph's Annotation system
const StateAnnotation = Annotation.Root({
  topic: Annotation<string>(),
  // aggregated search results as a string (concatenation of titles and content)
  searchResults: Annotation<string>(),
  summary: Annotation<string>(),
  draft: Annotation<string>(),
  refined: Annotation<string>(),
  iteration: Annotation<number>({ value: (_, next) => next, default: () => 0 }),
  seoData: Annotation<string>(), // raw JSON string from SEO prompt
  finalMdx: Annotation<string>(),
})

// Node: Conduct research using TavilySearchAPIRetriever
async function researchNode(state: typeof StateAnnotation.State) {
  // Use the retriever to fetch documents based on the topic.
  const results = await retriever.invoke(state.topic)

  // Aggregate the search results: join the title (from metadata) and the content.
  const aggregated = results
    .map((doc: any) => `${doc.metadata.title}: ${doc.pageContent}`)
    .join('\n')
  console.log('Research Results: ', aggregated)
  return { searchResults: aggregated }
}

// Node: Summarize the research results
async function summarizeNode(state: typeof StateAnnotation.State) {
  const prompt = researchSummaryPromptV1(state.topic, state.searchResults)
  const response = await llm.invoke([new HumanMessage(prompt)])
  console.log('Summary: ', response.content)
  return { summary: response.content }
}

// Node: Draft the blog post based on the research summary
async function draftNode(state: typeof StateAnnotation.State) {
  const prompt = draftBlogPromptV1(state.topic, state.summary)
  const response = await llm.invoke([new HumanMessage(prompt)])
  return { draft: response.content }
}

// Node: Refine the blog draft (iterative refinement)
async function refineNode(state: typeof StateAnnotation.State) {
  const prompt = refineBlogPromptV1(state.draft)
  const response = await llm.invoke([new HumanMessage(prompt)])
  return { refined: response.content, iteration: state.iteration + 1 }
}

// Node: SEO optimization (using JSON output parsing as before)
import { JsonOutputParser } from '@langchain/core/output_parsers'

// Define the expected SEO output type (optional)
interface SEOData {
  title: string
  meta: string
  content: string
}

async function seoNode(state: typeof StateAnnotation.State) {
  const prompt = seoPromptV1(state.topic, state.refined)
  const response = await llm.invoke([new HumanMessage(prompt)])

  // Use the JSON output parser to parse the response.
  const parser = new JsonOutputParser<SEOData>()
  const rawOutput =
    typeof response.content === 'string' ? response.content : response.content.toString()
  const seoData: SEOData = await parser.parse(rawOutput)
  return { seoData: JSON.stringify(seoData) }
}

// Node: Format final output in MDX
async function formatNode(state: typeof StateAnnotation.State) {
  const seoData = JSON.parse(state.seoData)
  const prompt = formatMdxPromptV1(seoData)
  const response = await llm.invoke([new HumanMessage(prompt)])
  return { finalMdx: response.content }
}

// Conditional edge function for iterative refinement.
// If iteration < MAX_ITERATIONS, loop back to refine; otherwise, proceed to SEO.
function afterRefine(state: typeof StateAnnotation.State) {
  return state.iteration < MAX_ITERATIONS ? 'refine' : 'seo'
}

// Build the LangGraph workflow
// Build the LangGraph workflow
const deepResearchGraph = new StateGraph(StateAnnotation)
  .addNode('research', researchNode)
  .addNode('summarize', summarizeNode)
  // Renamed the "draft" node to "createDraft" to avoid conflict with state attribute "draft"
  .addNode('createDraft', draftNode)
  .addNode('refine', refineNode)
  .addNode('seo', seoNode)
  .addNode('format', formatNode)
  .addEdge('__start__', 'research')
  .addEdge('research', 'summarize')
  // Update edge: from "summarize" to "createDraft"
  .addEdge('summarize', 'createDraft')
  // Update edge: from "createDraft" to "refine"
  .addEdge('createDraft', 'refine')
  .addConditionalEdges('refine', afterRefine, {
    refine: 'refine',
    seo: 'seo',
  })
  .addEdge('seo', 'format')
  .addEdge('format', '__end__')
  .compile()

// Main entry function for the Deep Research workflow
export async function runDeepResearch({ prompt, rawMdx }: HandlerParams): Promise<string> {
  // This workflow is only for new blogs; rawMdx is ignored.
  const state = await deepResearchGraph.invoke({ topic: prompt })
  return state.finalMdx
}


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/deepResearch/prompts.ts ==========

import { mdxFormattingInstructionsV1 } from '../../prompts/mdxFormatting'

/**
 * researchSummaryPrompt:
 * Given a topic and a list of search results, instructs the LLM to produce a summary.
 */
export const researchSummaryPromptV1 = (topic: string, searchResults: string): string =>
  `You are a research assistant. Summarize the following search results on the topic "${topic}" focusing on key insights and relevant information. Present a clear, concise summary.

Search Results:
${searchResults}

Summary:`

/**
 * draftBlogPrompt:
 * Given a topic and a research summary, instructs the LLM to draft a structured blog post.
 */
export const draftBlogPromptV1 = (topic: string, summary: string): string =>
  `Write a comprehensive blog post on the topic "${topic}" using the following research summary. Structure the blog post into three sections: Introduction, Main Points, and Conclusion. Ensure the content is detailed, engaging, and accurate.

Research Summary:
${summary}

Blog Post Draft:`

/**
 * refineBlogPrompt:
 * Given an existing blog draft, instructs the LLM to refine the content.
 */
export const refineBlogPromptV1 = (draft: string): string =>
  `You are an expert editor. Refine the following blog draft for clarity, accuracy, and flow. Do not change the overall structure (Introduction, Main Points, Conclusion). Provide an improved version of the blog post.

Draft:
${draft}

Refined Draft:`

/**
 * seoPrompt:
 * Given a refined blog draft, instructs the LLM to optimize the content for SEO.
 */
export const seoPromptV1 = (topic: string, refinedDraft: string): string =>
  `You are an SEO expert. Optimize the following blog draft for search engine visibility. Do the following:
1. Propose a catchy, SEO-optimized title for the blog post.
2. Write a meta description (around 155 characters) that highlights the blog's content.
3. Ensure the blog post is well-structured with appropriate headings (Introduction, Main Points, Conclusion) and includes relevant keywords naturally.
   
Topic: "${topic}"
Blog Draft:
${refinedDraft}

Provide the output in the following JSON format:
{
  "title": string,
  "meta": string,
  "content": string
}`

/**
 * formatMdxPrompt:
 * Given the SEO optimized content (with title and meta), instructs the LLM to format the final output in MDX.
 * It uses a pre-defined MDX formatting instruction that includes the current date and does not output an H1 title.
 */
export const formatMdxPromptV1 = (seoOptimized: {
  title: string
  meta: string
  content: string
}): string =>
  `Format the following blog content into a well-formatted MDX file.
${mdxFormattingInstructionsV1()}

Now format the content below into MDX:

Title: ${seoOptimized.title}

Meta Description: ${seoOptimized.meta}

Content:
${seoOptimized.content}`


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/chainOfThoughts/index.ts ==========

// File: /lib/workflows/chainOfThoughts/index.ts

import { ChatOpenAI } from '@langchain/openai'
import { HumanMessage } from '@langchain/core/messages'
import { RunnableSequence } from '@langchain/core/runnables'
import { StringOutputParser } from '@langchain/core/output_parsers'
import type { HandlerParams } from '../../types'
import {
  extractUserIntentionsPromptV1,
  blogPromptV1,
  modificationIntentionsPromptV1,
} from './prompts'
import { modificationPromptV1 } from '../../prompts/changes'

/**
 * runChainOfThoughts:
 * Runs a two-stage process that:
 * 1. Extracts clear user intentions from the raw input.
 * 2. Executes the desired action based on the mode:
 *    - In creation mode (rawMdx not provided), generates a new blog post.
 *    - In modification mode (rawMdx provided), updates the existing blog post.
 *
 * Decision logic is centralized here.
 *
 * @param params - Object containing:
 *   - prompt: The raw user input (topic or modification suggestions).
 *   - rawMdx (optional): Existing MDX content (for modification mode).
 * @returns A promise resolving to the final MDX output as a string.
 */
export async function runChainOfThoughts({ prompt, rawMdx }: HandlerParams): Promise<string> {
  const model = new ChatOpenAI({
    model: 'gpt-4o-mini',
    temperature: 0.7,
    apiKey: process.env.OPENAI_API_KEY,
  })

  // Decide mode: "modification" if rawMdx exists; otherwise "creation".
  const mode: 'creation' | 'modification' = rawMdx ? 'modification' : 'creation'

  if (mode === 'modification' && rawMdx) {
    // Modification mode: two-stage RunnableSequence.
    const chain = RunnableSequence.from([
      async () => {
        // Stage 1: Process the user's modification instructions.
        const reasoningPrompt = modificationIntentionsPromptV1(prompt)
        const reasoningResponse = await model.invoke([new HumanMessage(reasoningPrompt)])
        console.log('Extracted Reasoning:', reasoningResponse.content)
        return { reasoning: reasoningResponse.content }
      },
      async (input: { reasoning: string }) => {
        // Stage 2: Use the common modification prompt to update the blog post.
        const compositePrompt = modificationPromptV1(rawMdx, input.reasoning)
        const response = await model.invoke([new HumanMessage(compositePrompt)])
        console.log('Modification Response:', response.content)
        return { mdx: response.content }
      },
      (input: { mdx: string }) => input.mdx,
      new StringOutputParser(),
    ])
    return chain.invoke({})
  } else {
    // Creation mode: two-stage RunnableSequence.
    const chain = RunnableSequence.from([
      async (input: { topic: string }) => {
        // Stage 1: Extract clear user intentions from the topic.
        const extractionPrompt = extractUserIntentionsPromptV1(input.topic)
        const extractionResponse = await model.invoke([new HumanMessage(extractionPrompt)])
        console.log('Extracted Intention:', extractionResponse.content)
        return { intention: extractionResponse.content }
      },
      async (input: { intention: string }) => {
        // Stage 2: Use the extracted intention to generate a new blog post.
        const compositePrompt = blogPromptV1(input.intention)
        const response = await model.invoke([new HumanMessage(compositePrompt)])
        console.log('Creation Response:', response.content)
        return { mdx: response.content }
      },
      (input: { mdx: string }) => input.mdx,
      new StringOutputParser(),
    ])
    return chain.invoke({ topic: prompt })
  }
}


========== /home/serg/node/agentic-blog-writer-nextjs/lib/workflows/chainOfThoughts/prompts.ts ==========

// File: /lib/workflows/chainOfThoughts/prompts.ts

import { mdxFormattingInstructionsV1 } from '../../prompts/mdxFormatting'

/**
 * extractUserIntentionsPromptV1:
 * Returns a prompt that instructs the LLM to extract a clear, unambiguous statement of user intentions
 * from the input. (For new blog creation.)
 */
export const extractUserIntentionsPromptV1 = (input: string): string =>
  `Analyze the following input and extract a clear, concise statement of user intentions.
Output only the extracted intention, with no additional commentary or explanation.
Input: ${input}`

/**
 * modificationReasoningPromptV1:
 * Given the user's modification suggestions, returns a prompt that asks the LLM to analyze
 * and explain the intended changes.
 */
export const modificationIntentionsPromptV1 = (changes: string): string =>
  `Please analyze the following modification suggestions and explain in detail what changes are intended.
Modifications: ${changes}`

/**
 * blogPromptV1:
 * Given an extracted intention, returns a prompt to generate a complete blog post in MDX format.
 */
export const blogPromptV1 = (intention: string): string =>
  `Using the following extracted intention:
  
${intention}

Generate a comprehensive blog post in MDX format.
${mdxFormattingInstructionsV1()}`


========== /home/serg/node/agentic-blog-writer-nextjs/lib/prompts/changes.ts ==========

// File: /lib/prompts/changes.ts

export const modificationPromptV1 = (rawMdx: string, changes: string): string =>
  `Below is an existing blog post in MDX format (including YAML frontmatter):
${rawMdx}

Using the following modifications, update the blog post accordingly.
Ensure that the YAML frontmatter (title, date, tags, draft, summary) is preserved.
Return only the modified MDX content (including the YAML frontmatter) with no additional commentary or explanation.
Suggested modifications: ${changes}`


========== /home/serg/node/agentic-blog-writer-nextjs/lib/prompts/mdxFormatting.ts ==========

// File: /lib/prompts/mdxFormatting.ts

export function mdxFormattingInstructionsV1(): string {
  const currentDate = new Date().toISOString().slice(0, 10)
  return `Ensure the final output is a well-formatted MDX file.
It must begin with a YAML frontmatter block containing **only** the following fields:
- title: A catchy, SEO-optimized title.
- date: "${currentDate}" (this specific date must be used).
- tags: An array of relevant tags.
- draft: false.
- description: A concise, SEO-friendly description.

Do not include any additional fields 

Example frontmatter:
---
title: "Markdown Guide"
date: "${currentDate}"
tags: ["github", "guide"]
draft: false
description: "Markdown cheatsheet for all your blogging needs - headers, lists, images, tables and more!"
---

**Important:**
- Do not include a Markdown H1 title (e.g., "# Title") in the main content; the title should only appear in the frontmatter.
- The date must be exactly as provided above.

After the frontmatter, output the main blog content in Markdown.`
}


========== /home/serg/node/agentic-blog-writer-nextjs/lib/contentlayer/rebuildContentlayer.ts ==========

// lib/contentlayer/rebuildContentlayer.ts

import { exec } from 'child_process'

export async function rebuildContentlayer() {
  // Only run in production to avoid unnecessary rebuilds in dev
  if (process.env.NODE_ENV !== 'production') {
    return
  }
  try {
    // Execute the Contentlayer CLI to rebuild content
    await new Promise<void>((resolve, reject) => {
      exec('npx contentlayer2 build', (error, stdout, stderr) => {
        if (error) {
          return reject(error)
        }
        console.log(stdout) // optional: log Contentlayer output
        resolve()
      })
    })
    console.log('✅ Contentlayer regeneration complete.')
  } catch (err) {
    console.error('❌ Failed to rebuild Contentlayer:', err)
  }
}

